python -m torch.distributed.launch \
--nproc_per_node=8 transformers/examples/seq2seq/final_run_summarization_mbart.py \
--model_name_or_path facebook/mbart-large-50 \
--max_target_length 1024 \
--per_device_train_batch_size 1 \
--output_dir saved/ \
--overwrite_output_dir \
--do_train \
--train_file long_wiki_fr/train.json \
--text_column text \
--summary_column summary \
--sharded_ddp zero_dp_2 \
--num_train_epochs 10 \
--save_total_limit 10 \
--save_steps 1000 \
--fp16 \
--do_eval \
--validation_file long_wiki_fr/valid.json \
--per_device_eval_batch_size 1 \
--evaluation_strategy epoch 

